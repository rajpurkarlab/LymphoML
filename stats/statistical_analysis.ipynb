{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3289978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import itertools\n",
    "import os\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bddd9a",
   "metadata": {},
   "source": [
    "## Computing CIs for Overall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd11de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, path):\n",
    "        self.name = os.path.splitext(os.path.basename(path))[0]\n",
    "        self.type = os.path.basename(os.path.dirname(path))\n",
    "        self.df = pd.read_csv(path, keep_default_na=False)\n",
    "\n",
    "\n",
    "def single_replicate_all_models(models, metric_str, replicate_num):\n",
    "    performances = {}\n",
    "    for model in models:\n",
    "        gt = model.df[\"label\"]\n",
    "        preds = model.df[\"prediction\"]\n",
    "        sample_ids = np.random.choice(len(gt), size=len(gt), replace=True)\n",
    "        \n",
    "        # Uncomment this for testing: \n",
    "        # Set sample_ids equal to gt.index and we should get metrics on the full test set.\n",
    "        # sample_ids = gt.index\n",
    "        \n",
    "        y_true = gt.iloc[sample_ids].to_numpy().ravel()\n",
    "        y_score = preds.iloc[sample_ids].to_numpy().ravel()\n",
    "        if metric_str == 'acc':\n",
    "            performance = sklearn.metrics.accuracy_score(y_true, y_score)\n",
    "        elif metric_str == 'f1':\n",
    "            performance = sklearn.metrics.f1_score(y_true, y_score, average=\"weighted\")\n",
    "        else:\n",
    "            raise ValueError('Metric Not Defined')\n",
    "        performances[model.name] = performance\n",
    "    return performances\n",
    "\n",
    "\n",
    "def multiple_replicate_all_models(models, num_replicates, metric_str):\n",
    "    func = partial(single_replicate_all_models, models, metric_str)\n",
    "    results = list(map(func, range(num_replicates)))\n",
    "    return results\n",
    "\n",
    "\n",
    "class ConfidenceGenerator():\n",
    "    def __init__(self, confidence_level):\n",
    "        self.records = []\n",
    "        self.confidence_level = confidence_level\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_cis(series, confidence_level):\n",
    "        sorted_perfs = series.sort_values()\n",
    "        lower_index = int(confidence_level/2 * len(sorted_perfs)) - 1\n",
    "        upper_index = int((1 - confidence_level/2) * len(sorted_perfs)) - 1\n",
    "        lower = round(sorted_perfs.iloc[lower_index], 3)\n",
    "        upper = round(sorted_perfs.iloc[upper_index], 3)\n",
    "        mean = round(sorted_perfs.mean(), 3)\n",
    "        return lower, mean, upper\n",
    "\n",
    "    def create_ci_record(self, perfs, name, perf_type):\n",
    "        lower, mean, upper = ConfidenceGenerator.compute_cis(\n",
    "            perfs, self.confidence_level)\n",
    "        record = {\"name\": name,\n",
    "                  \"type\": perf_type,\n",
    "                  \"lower\": lower,\n",
    "                  \"mean\": mean,\n",
    "                  \"upper\": upper}\n",
    "        self.records.append(record)\n",
    "\n",
    "    def generate_cis(self, df):\n",
    "        for name in df.columns:\n",
    "            self.create_ci_record(df[name], name, 'individual')\n",
    "            \n",
    "        for name1, name2 in itertools.combinations(df.columns, 2):\n",
    "            model_diffs = df[name1] - df[name2]\n",
    "            self.create_ci_record(model_diffs, f\"{name1}-{name2}\", 'difference')\n",
    "\n",
    "        df = pd.DataFrame.from_records(self.records)\n",
    "        return df\n",
    "\n",
    "def run_stage_1_models(models, num_replicates, metric_str, save_path):\n",
    "    evaluations = multiple_replicate_all_models(\n",
    "        models,\n",
    "        num_replicates,\n",
    "        metric_str)\n",
    "    df = pd.DataFrame.from_records(evaluations)\n",
    "    if save_path:\n",
    "        df.to_csv(f\"{save_path}.csv\", index=False)\n",
    "\n",
    "\n",
    "def run_stage1(num_replicates, metric_str, read_path, save_path=None):\n",
    "    if metric_str in ['f1', 'acc']:\n",
    "        model_paths = glob.glob(f'{read_path}/*.csv', recursive=True)\n",
    "    else:\n",
    "        raise ValueError(\"Metric path not defined\")\n",
    "\n",
    "    models = [Model(path) for path in model_paths]\n",
    "\n",
    "    run_stage_1_models(models, num_replicates, metric_str, save_path)\n",
    "\n",
    "\n",
    "def run_stage2(confidence_level, read_path, save_path=None):\n",
    "    perfs = pd.read_csv(f\"{read_path}.csv\")\n",
    "    cb = ConfidenceGenerator(confidence_level=confidence_level)\n",
    "    df = cb.generate_cis(perfs)\n",
    "    if save_path:\n",
    "        df.to_csv(f\"{save_path}.csv\", index=False)\n",
    "\n",
    "def main(working_dir, num_replicates):\n",
    "    metrics = ['acc', 'f1']\n",
    "\n",
    "    for metric_str in metrics:\n",
    "        stage_1_save_path = f'{working_dir}/stats/raw_{metric_str}_95'\n",
    "        run_stage1(num_replicates=num_replicates,\n",
    "                   metric_str=metric_str,\n",
    "                   read_path=f'{working_dir}/predictions',\n",
    "                   save_path=stage_1_save_path,\n",
    "                   )\n",
    "\n",
    "        stage_2_save_path = f'{working_dir}/stats/processed_{metric_str}_95'\n",
    "        run_stage2(read_path=stage_1_save_path,\n",
    "                   save_path=stage_2_save_path,\n",
    "                   confidence_level=0.05,\n",
    "                  )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_replicates = 1000\n",
    "    working_dir = \"/deep/group/aihc-bootcamp-fall2021/lymphoma/results\"\n",
    "    main(working_dir, num_replicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846f81e",
   "metadata": {},
   "source": [
    "## Computing CIs for Per-class Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1108ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_label = {0: 'DLBCL',\n",
    "                  1: 'HL',\n",
    "                  2: 'Agg BCL',\n",
    "                  3: 'FL',\n",
    "                  4: 'MCL',\n",
    "                  5: 'MZL',\n",
    "                  6: 'NKTCL',\n",
    "                  7: 'TCL'}\n",
    "\n",
    "def compute_per_class_f1(y_true, y_score, per_class):\n",
    "    if per_class not in y_true:\n",
    "        if label_to_label[per_class] not in y_true:\n",
    "            score = np.NaN\n",
    "        else:\n",
    "            score = f1_score(y_true, y_score, average=None, labels=['DLBCL','HL','Agg BCL','FL','MCL','MZL','NKTCL','TCL'])[per_class]\n",
    "    else:\n",
    "        score = f1_score(y_true, y_score, average=None, labels=[0, 1, 2, 3, 4, 5, 6, 7])[per_class]\n",
    "    return score\n",
    "\n",
    "def compute_per_class_acc(df, per_class):\n",
    "    TP = len(df[(df['label']==per_class) & (df['prediction']==per_class)])\n",
    "    TN = len(df[(df['label']!=per_class) & (df['prediction']!=per_class)])\n",
    "    FP = len(df[(df['label']!=per_class) & (df['prediction']==per_class)])\n",
    "    FN = len(df[(df['label']==per_class) & (df['prediction']!=per_class)])\n",
    "\n",
    "    \n",
    "    return (TP+TN)/(TP+TN+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf33d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, path, per_class):\n",
    "        self.name = os.path.splitext(os.path.basename(path))[0]\n",
    "        self.type = os.path.basename(os.path.dirname(path))\n",
    "        self.df = pd.read_csv(path, keep_default_na=False)\n",
    "        self.per_class = per_class\n",
    "\n",
    "\n",
    "def single_replicate_all_models(models, metric_str, replicate_num):\n",
    "    performances = {}\n",
    "    for model in models:\n",
    "        gt = model.df[\"label\"]\n",
    "        preds = model.df[\"prediction\"]\n",
    "        sample_ids = np.random.choice(len(gt), size=len(gt), replace=True)\n",
    "        \n",
    "        # Uncomment this for testing: \n",
    "        # Set sample_ids equal to gt.index and we should get metrics on the full test set.\n",
    "        # sample_ids = gt.index\n",
    "        \n",
    "        y_true = gt.iloc[sample_ids].to_numpy().ravel()\n",
    "        y_score = preds.iloc[sample_ids].to_numpy().ravel()\n",
    "        temp_model = pd.DataFrame({'label': y_true, 'prediction': y_score})\n",
    "        if metric_str == 'acc':\n",
    "            performance = compute_per_class_acc(temp_model, model.per_class)\n",
    "        elif metric_str == 'f1':\n",
    "            performance = compute_per_class_f1(y_true, y_score, model.per_class)\n",
    "        else:\n",
    "            raise ValueError('Metric Not Defined')\n",
    "        performances[model.name] = performance\n",
    "    return performances\n",
    "\n",
    "\n",
    "def multiple_replicate_all_models(models, num_replicates, metric_str):\n",
    "    func = partial(single_replicate_all_models, models, metric_str)\n",
    "    results = list(map(func, range(num_replicates)))\n",
    "    return results\n",
    "\n",
    "\n",
    "class ConfidenceGenerator():\n",
    "    def __init__(self, confidence_level):\n",
    "        self.records = []\n",
    "        self.confidence_level = confidence_level\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_cis(series, confidence_level):\n",
    "        series = series.dropna()\n",
    "        if len(series) == 0:\n",
    "            return 0, 0, 0\n",
    "        sorted_perfs = series.sort_values()\n",
    "        lower_index = int(confidence_level/2 * len(sorted_perfs)) - 1\n",
    "        upper_index = int((1 - confidence_level/2) * len(sorted_perfs)) - 1\n",
    "        print(len(sorted_perfs), lower_index, upper_index)\n",
    "        lower = round(sorted_perfs.iloc[lower_index], 3)\n",
    "        upper = round(sorted_perfs.iloc[upper_index], 3)\n",
    "        mean = round(sorted_perfs.mean(), 3)\n",
    "        return lower, mean, upper\n",
    "\n",
    "    def create_ci_record(self, perfs, name, perf_type):\n",
    "        lower, mean, upper = ConfidenceGenerator.compute_cis(\n",
    "            perfs, self.confidence_level)\n",
    "        record = {\"name\": name,\n",
    "                  \"type\": perf_type,\n",
    "                  \"lower\": lower,\n",
    "                  \"mean\": mean,\n",
    "                  \"upper\": upper}\n",
    "        self.records.append(record)\n",
    "\n",
    "    def generate_cis(self, df):\n",
    "        for name in df.columns:\n",
    "            self.create_ci_record(df[name], name, 'individual')\n",
    "            \n",
    "        for name1, name2 in itertools.combinations(df.columns, 2):\n",
    "            model_diffs = df[name1] - df[name2]\n",
    "            self.create_ci_record(model_diffs, f\"{name1}-{name2}\", 'difference')\n",
    "\n",
    "        df = pd.DataFrame.from_records(self.records)\n",
    "        return df\n",
    "\n",
    "def run_stage_1_models(models, num_replicates, metric_str, save_path):\n",
    "    evaluations = multiple_replicate_all_models(\n",
    "        models,\n",
    "        num_replicates,\n",
    "        metric_str)\n",
    "    df = pd.DataFrame.from_records(evaluations)\n",
    "    if save_path:\n",
    "        df.to_csv(f\"{save_path}.csv\", index=False)\n",
    "\n",
    "\n",
    "def run_stage1(num_replicates, metric_str, read_path, save_path=None, per_class=None):\n",
    "    if metric_str in ['f1', 'acc']:\n",
    "        model_paths = glob.glob(f'{read_path}/*.csv', recursive=True)\n",
    "    else:\n",
    "        raise ValueError(\"Metric path not defined\")\n",
    "\n",
    "    models = [Model(path, per_class) for path in model_paths]\n",
    "\n",
    "    run_stage_1_models(models, num_replicates, metric_str, save_path)\n",
    "\n",
    "\n",
    "def run_stage2(confidence_level, read_path, save_path=None):\n",
    "    perfs = pd.read_csv(f\"{read_path}.csv\")\n",
    "    cb = ConfidenceGenerator(confidence_level=confidence_level)\n",
    "    df = cb.generate_cis(perfs)\n",
    "    if save_path:\n",
    "        df.to_csv(f\"{save_path}.csv\", index=False)\n",
    "\n",
    "def main(working_dir, num_replicates, per_class):\n",
    "    metrics = ['acc', 'f1']\n",
    "\n",
    "    for metric_str in metrics:\n",
    "        stage_1_save_path = f'{working_dir}/stats/raw_{metric_str}'\n",
    "        run_stage1(num_replicates=num_replicates,\n",
    "                   metric_str=metric_str,\n",
    "                   read_path=f'{working_dir}/predictions',\n",
    "                   save_path=stage_1_save_path,\n",
    "                   per_class=per_class,\n",
    "                   )\n",
    "\n",
    "        stage_2_save_path = f'{working_dir}/stats/processed_{metric_str}_class_{per_class}'\n",
    "        run_stage2(read_path=stage_1_save_path,\n",
    "                   save_path=stage_2_save_path,\n",
    "                   confidence_level=0.05,\n",
    "                  )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    per_class = 0\n",
    "    num_replicates = 1000\n",
    "    working_dir = f\"/deep/group/aihc-bootcamp-fall2021/lymphoma/results\"\n",
    "    main(working_dir, num_replicates, per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d6962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3dfc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
